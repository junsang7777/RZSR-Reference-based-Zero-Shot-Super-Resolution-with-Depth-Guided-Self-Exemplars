import time
import cv2
from sklearn.feature_extraction import image
import heapq
import matplotlib.pyplot as plt
from sklearn.base import TransformerMixin
import numpy as np
from scipy.stats import multivariate_normal
import warnings
from sklearn.base import BaseEstimator, ClusterMixin, TransformerMixin
from sklearn.metrics.pairwise import (
    pairwise_distances,
    pairwise_distances_argmin,
)
from sklearn.utils import check_array, check_random_state
from sklearn.utils.extmath import stable_cumsum
from sklearn.utils.validation import check_is_fitted
from sklearn.exceptions import ConvergenceWarning


def idx1d_to_idx2d(idx, M, N): #인덱스 디멘션 맞추기
    j = int(np.floor(idx/M))
    i = idx - j*M
    return [i, j]

def idx2d_to_idx1d(i, j, M, N):
    idx = int(M*j + i)
    return idx



class KMedoids(BaseEstimator, ClusterMixin, TransformerMixin):
    """k-medoids clustering.
    Read more in the :ref:`User Guide <k_medoids>`.
    Parameters
    ----------
    n_clusters : int, optional, default: 8
        The number of clusters to form as well as the number of medoids to
        generate.
    metric : string, or callable, optional, default: 'euclidean'
        What distance metric to use. See :func:metrics.pairwise_distances
    init : {'random', 'heuristic', 'k-medoids++'}, optional, default: 'heuristic'
        Specify medoid initialization method. 'random' selects n_clusters
        elements from the dataset. 'heuristic' picks the n_clusters points
        with the smallest sum distance to every other point. 'k-medoids++'
        follows an approach based on k-means++_, and in general, gives initial
        medoids which are more separated than those generated by the other methods.
        .. _k-means++: https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf
    max_iter : int, optional, default : 300
        Specify the maximum number of iterations when fitting.
    random_state : int, RandomState instance or None, optional
        Specify random state for the random number generator. Used to
        initialise medoids when init='random'.
    Attributes
    ----------
    cluster_centers_ : array, shape = (n_clusters, n_features)
            or None if metric == 'precomputed'
        Cluster centers, i.e. medoids (elements from the original dataset)
    medoid_indices_ : array, shape = (n_clusters,)
        The indices of the medoid rows in X
    labels_ : array, shape = (n_samples,)
        Labels of each point
    inertia_ : float
        Sum of distances of samples to their closest cluster center.
    Examples
    --------
    8.0
    See scikit-learn-extra/examples/plot_kmedoids_digits.py for examples
    of KMedoids with various distance metrics.
    References
    ----------
    Maranzana, F.E., 1963. On the location of supply points to minimize
      transportation costs. IBM Systems Journal, 2(2), pp.129-135.
    Park, H.S.and Jun, C.H., 2009. A simple and fast algorithm for K-medoids
      clustering.  Expert systems with applications, 36(2), pp.3336-3341.
    See also
    --------
    KMeans
        The KMeans algorithm minimizes the within-cluster sum-of-squares
        criterion. It scales well to large number of samples.
    Notes
    -----
    Since all pairwise distances are calculated and stored in memory for
    the duration of fit, the space complexity is O(n_samples ** 2).

    """
    def __init__(
        self,
        n_clusters=8,
        metric="euclidean",
        init="heuristic",
        max_iter=300,
        random_state=None,
    ):
        self.n_clusters = n_clusters
        self.metric = metric
        self.init = init
        self.max_iter = max_iter
        self.random_state = random_state

    def _check_nonnegative_int(self, value, desc):
        """Validates if value is a valid integer > 0"""

        if (
            value is None
            or value <= 0
            or not isinstance(value, (int, np.integer))
        ):
            raise ValueError(
                "%s should be a nonnegative integer. "
                "%s was given" % (desc, value)
            )

    def _check_init_args(self):
        """Validates the input arguments. """
        # Check n_clusters and max_iter
        self._check_nonnegative_int(self.n_clusters, "n_clusters")
        self._check_nonnegative_int(self.max_iter, "max_iter")
        # Check init
        init_methods = ["random", "heuristic", "k-medoids++"]
        if self.init not in init_methods:
            raise ValueError(
                "init needs to be one of "
                + "the following: "
                + "%s" % init_methods
            )

    def fit(self, X, y=None):
        """Fit K-Medoids to the provided data.
        Parameters
        ----------
        X : {array-like, sparse matrix}, shape = (n_samples, n_features), \
                or (n_samples, n_samples) if metric == 'precomputed'
            Dataset to cluster.
        y : Ignored
        Returns
        -------
        self
        """
        random_state_ = check_random_state(self.random_state)
        self._check_init_args()
        X = check_array(X, accept_sparse=["csr", "csc"])
        if self.n_clusters > X.shape[0]:
            raise ValueError(
                "The number of medoids (%d) must be less "
                "than the number of samples %d."
                % (self.n_clusters, X.shape[0])
            )
        D = pairwise_distances(X, metric=self.metric)
        medoid_idxs = self._initialize_medoids(
            D, self.n_clusters, random_state_
        )
        labels = None
        # Continue the algorithm as long as
        # the medoids keep changing and the maximum number
        # of iterations is not exceeded
        for self.n_iter_ in range(0, self.max_iter):
            old_medoid_idxs = np.copy(medoid_idxs)
            labels = np.argmin(D[medoid_idxs, :], axis=0)

            # Update medoids with the new cluster indices
            self._update_medoid_idxs_in_place(D, labels, medoid_idxs)
            if np.all(old_medoid_idxs == medoid_idxs):
                break
            elif self.n_iter_ == self.max_iter - 1:
                warnings.warn(
                    "Maximum number of iteration reached before "
                    "convergence. Consider increasing max_iter to "
                    "improve the fit.",
                    ConvergenceWarning,
                )
        # Set the resulting instance variables.
        if self.metric == "precomputed":
            self.cluster_centers_ = None
        else:
            self.cluster_centers_ = X[medoid_idxs]
        # Expose labels_ which are the assignments of
        # the training data to clusters
        self.labels_ = labels
        self.medoid_indices_ = medoid_idxs
        self.inertia_ = self._compute_inertia(self.transform(X))
        # Return self to enable method chaining
        return self

    def _update_medoid_idxs_in_place(self, D, labels, medoid_idxs):
        """In-place update of the medoid indices"""
        # Update the medoids for each cluster
        for k in range(self.n_clusters):
            # Extract the distance matrix between the data points
            # inside the cluster k
            cluster_k_idxs = np.where(labels == k)[0]

            if len(cluster_k_idxs) == 0:
                warnings.warn(
                    "Cluster {k} is empty! "
                    "self.labels_[self.medoid_indices_[{k}]] "
                    "may not be labeled with "
                    "its corresponding cluster ({k}).".format(k=k)
                )
                continue
            in_cluster_distances = D[
                cluster_k_idxs, cluster_k_idxs[:, np.newaxis]
            ]

            # Calculate all costs from each point to all others in the cluster
            in_cluster_all_costs = np.sum(in_cluster_distances, axis=1)

            min_cost_idx = np.argmin(in_cluster_all_costs)
            min_cost = in_cluster_all_costs[min_cost_idx]
            curr_cost = in_cluster_all_costs[
                np.argmax(cluster_k_idxs == medoid_idxs[k])
            ]
            # Adopt a new medoid if its distance is smaller then the current
            if min_cost < curr_cost:
                medoid_idxs[k] = cluster_k_idxs[min_cost_idx]

    def transform(self, X):
        """Transforms X to cluster-distance space.
        Parameters
        ----------
        X : {array-like, sparse matrix}, shape (n_query, n_features), \
                or (n_query, n_indexed) if metric == 'precomputed'
            Data to transform.
        Returns
        -------
        X_new : {array-like, sparse matrix}, shape=(n_query, n_clusters)
            X transformed in the new space of distances to cluster centers.
        """
        X = check_array(X, accept_sparse=["csr", "csc"])

        if self.metric == "precomputed":
            check_is_fitted(self, "medoid_indices_")
            return X[:, self.medoid_indices_]
        else:
            check_is_fitted(self, "cluster_centers_")
            Y = self.cluster_centers_
            return pairwise_distances(X, Y=Y, metric=self.metric)

    def predict(self, X):
        """Predict the closest cluster for each sample in X.
        Parameters
        ----------
        X : {array-like, sparse matrix}, shape (n_query, n_features), \
                or (n_query, n_indexed) if metric == 'precomputed'
            New data to predict.
        Returns
        -------
        labels : array, shape = (n_query,)
            Index of the cluster each sample belongs to.
        """
        X = check_array(X, accept_sparse=["csr", "csc"])
        if self.metric == "precomputed":
            check_is_fitted(self, "medoid_indices_")
            return np.argmin(X[:, self.medoid_indices_], axis=1)
        else:
            check_is_fitted(self, "cluster_centers_")
            # Return data points to clusters based on which cluster assignment
            # yields the smallest distance
            return pairwise_distances_argmin(
                X, Y=self.cluster_centers_, metric=self.metric
            )

    def _compute_inertia(self, distances):
        """Compute inertia of new samples. Inertia is defined as the sum of the
        sample distances to closest cluster centers.
        Parameters
        ----------
        distances : {array-like, sparse matrix}, shape=(n_samples, n_clusters)
            Distances to cluster centers.
        Returns
        -------
        Sum of sample distances to closest cluster centers.
        """
        # Define inertia as the sum of the sample-distances
        # to closest cluster centers
        inertia = np.sum(np.min(distances, axis=1))
        return inertia

    def _initialize_medoids(self, D, n_clusters, random_state_):
        """Select initial mediods when beginning clustering."""
        if self.init == "random":  # Random initialization
            # Pick random k medoids as the initial ones.
            medoids = random_state_.choice(len(D), n_clusters)
        elif self.init == "k-medoids++":
            medoids = self._kpp_init(D, n_clusters, random_state_)
        elif self.init == "heuristic":  # Initialization by heuristic
            # Pick K first data points that have the smallest sum distance
            # to every other point. These are the initial medoids.
            medoids = np.argpartition(np.sum(D, axis=1), n_clusters - 1)[
                :n_clusters
            ]
        #else:
        #    raise ValueError(f"init value '{self.init}' not recognized")
        return medoids

    # Copied from sklearn.cluster.k_means_._k_init
    def _kpp_init(self, D, n_clusters, random_state_, n_local_trials=None):
        """Init n_clusters seeds with a method similar to k-means++
        Parameters
        -----------
        D : array, shape (n_samples, n_samples)
            The distance matrix we will use to select medoid indices.
        n_clusters : integer
            The number of seeds to choose
        random_state : RandomState
            The generator used to initialize the centers.
        n_local_trials : integer, optional
            The number of seeding trials for each center (except the first),
            of which the one reducing inertia the most is greedily chosen.
            Set to None to make the number of trials depend logarithmically
            on the number of seeds (2+log(k)); this is the default.
        Notes
        -----
        Selects initial cluster centers for k-medoid clustering in a smart way
        to speed up convergence. see: Arthur, D. and Vassilvitskii, S.
        "k-means++: the advantages of careful seeding". ACM-SIAM symposium
        on Discrete algorithms. 2007
        Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,
        which is the implementation used in the aforementioned paper.
        """
        n_samples, _ = D.shape
        centers = np.empty(n_clusters, dtype=int)
        # Set the number of local seeding trials if none is given
        if n_local_trials is None:
            # This is what Arthur/Vassilvitskii tried, but did not report
            # specific results for other than mentioning in the conclusion
            # that it helped.
            n_local_trials = 2 + int(np.log(n_clusters))
        center_id = random_state_.randint(n_samples)
        centers[0] = center_id
        # Initialize list of closest distances and calculate current potential
        closest_dist_sq = D[centers[0], :] ** 2
        current_pot = closest_dist_sq.sum()
        # pick the remaining n_clusters-1 points
        for cluster_index in range(1, n_clusters):
            rand_vals = (
                random_state_.random_sample(n_local_trials) * current_pot
            )
            candidate_ids = np.searchsorted(
                stable_cumsum(closest_dist_sq), rand_vals
            )
            # Compute distances to center candidates
            distance_to_candidates = D[candidate_ids, :] ** 2
            # Decide which candidate is the best
            best_candidate = None
            best_pot = None
            best_dist_sq = None
            for trial in range(n_local_trials):
                # Compute potential when including center candidate
                new_dist_sq = np.minimum(
                    closest_dist_sq, distance_to_candidates[trial]
                )
                new_pot = new_dist_sq.sum()
                # Store result if it is the best local trial so far
                if (best_candidate is None) or (new_pot < best_pot):
                    best_candidate = candidate_ids[trial]
                    best_pot = new_pot
                    best_dist_sq = new_dist_sq
            centers[cluster_index] = best_candidate
            current_pot = best_pot
            closest_dist_sq = best_dist_sq
        return centers


#%%


import torch
import torch.nn as nn
import torch.nn.functional as F

from torchvision import models
import torchvision.transforms as transforms

import cv2
from sklearn.feature_extraction import image
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

class MeanShift(nn.Conv2d):
    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):
        super(MeanShift, self).__init__(3, 3, kernel_size=1)
        std = torch.Tensor(rgb_std)
        self.weight.data = torch.eye(3).view(3, 3, 1, 1)
        self.weight.data.div_(std.view(3, 1, 1, 1))
        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)
        self.bias.data.div_(std)
        # self.requires_grad = False
        self.weight.requires_grad = False
        self.bias.requires_grad = False


# pretrained vgg19 사용하는 방식은 https://github.com/researchmm/TTSR 에서 참고함.
class Vgg19FeatureExtractor(torch.nn.Module):
    def __init__(self, vgg_range=12, rgb_range=1):
        super(Vgg19FeatureExtractor, self).__init__()

        # use vgg19 weights to initialize
        vgg_pretrained_features = models.vgg19(pretrained=True).features
        self.slice1 = torch.nn.Sequential()

        for x in range(vgg_range):
            self.slice1.add_module(str(x), vgg_pretrained_features[x])

        # 학습을 시켜주지 않기때문에 grad 를 모두 없앤다.
        for param in self.slice1.parameters():
            param.requires_grad = False

        vgg_mean = (0.485, 0.456, 0.406)
        vgg_std = (0.229 * rgb_range, 0.224 * rgb_range, 0.225 * rgb_range)
        self.sub_mean = MeanShift(rgb_range, vgg_mean, vgg_std)

    def forward(self, x):
        x = self.sub_mean(x)
        x = self.slice1(x)
        x_lv1 = x
        return x_lv1


def tensor2cv2(tensor):
    # 3 channel 영상을 RGB 영상으로 만들어준다.
    # 또는 1 channel 영상을 grayscale 영상으로 만들어준다.

    # 0,1 -> 0,255
    npimg = tensor.cpu().numpy() * 255

    # 영상 후처리.
    npimg = np.around(npimg)
    npimg = npimg.clip(0, 255)
    npimg = npimg.astype(np.uint8)

    # 3 channel 일 경우
    if len(npimg.shape) == 3:
        # tensor 를 cv2 로 바꾸기 위해 channel 순서를 바꿔줌 (channel, w, h) -> (w, h, channel)
        npimg = np.transpose(npimg, (1, 2, 0))
    # 1 channel 영상일 경우

    return npimg

#%%
def RGB_patch_tensors_extractor(img_tensor, window_size): # must color image
    print("RGB_patch_tensors_extractor===============================")
    print(img_tensor.shape)
    channel = img_tensor.shape[1]
    img_tensor_unfold = F.unfold(img_tensor, kernel_size=(window_size, window_size), padding=window_size//2).squeeze()
    print(img_tensor_unfold.shape)
    img_tensor_unfold = img_tensor_unfold.permute((1, 0))
    print(img_tensor_unfold.shape)
    patches = img_tensor_unfold.view(img_tensor_unfold.shape[0],
                                     channel,
                                     window_size,
                                     window_size)
    print(patches.shape)
    print("===========================================================")
    return patches


class Patchmatch_VGG_MATCHING:
    def __init__(self, depth_img, input_img, crop_size, scale_factor):
        """
        depth_img = input_img 입력 영상
        """
        self.scale_factor = scale_factor

        self.PATCH_BIN = 10
        self.PATCH_BIN_4scale = 5

        self.crop_size = crop_size
        self.depth_img = depth_img
        [M, N] = self.depth_img.shape  # image size [ Height , Width ]
        self.depth_scale_img = cv2.resize(self.depth_img, dsize=None, fx=1/scale_factor[0][1],fy=1/scale_factor[0][0],interpolation=cv2.INTER_CUBIC)  # x0.5 scale depth image

        self.WINDOW_SIZE = int(self.crop_size/(np.mean(self.scale_factor[0])))-1  # Patch size
        self.WINDOW_PAD = int(self.WINDOW_SIZE/2) - 1  # Padding size
        self.HARD_THRESHOLD = 0.1
        self.input = input_img
        self.img_scale = cv2.resize(input_img,dsize=None, fx=1/scale_factor[0][1],fy=1/scale_factor[0][0],interpolation=cv2.INTER_CUBIC) # x0.5 scale image for making patches
        ##########
        [M_scale, N_scale] = self.depth_scale_img.shape # 1/2 image shape
        #############################################3
        self.img_4scale = cv2.resize(input_img, dsize=None, fx=(1/scale_factor[0][1])*(1/scale_factor[0][1]),fy=(1/scale_factor[0][0])*(1/scale_factor[0][0]),interpolation=cv2.INTER_CUBIC)

        totensor = transforms.ToTensor()
        self.img_tensor = totensor(self.input)
        self.img_2scale_tensor = totensor(self.img_scale)
        self.img_4scale_tensor = totensor(self.img_4scale)

        self.vgg_range = 12
        if self.vgg_range > 0:  # -> vgg feature 를 뽑는다.
            # Vgg19FeatureExtractor 를 선언하는 것 만으로도 gpu 용량을 차지하는 듯 하다?
            self.Vgg19FeatureExtractor = Vgg19FeatureExtractor(vgg_range=self.vgg_range, rgb_range=1).cuda()

            # vgg feature 에서 win size 를 즉정한다. 측정을 위해 win_size 크기의 임의의 패치를 모델에 통과시켜서 output size 를 관찰한다.
            temp_patch = torch.zeros(3, self.WINDOW_SIZE, self.WINDOW_SIZE)  # vgg19 의 입력은 3 channel 이다.
            print('temp_patch.shape :', temp_patch.shape)
            vgg_win_size = self.Vgg19FeatureExtractor(temp_patch.unsqueeze(0).cuda()).shape[-1]
            if vgg_win_size % 2 == 0:  # win size 가 홀수인게 편한 듯 하다.-> 짝수면 홀수로 바꿔줌.
                vgg_win_size += 1
            #print('vgg_win_size :', vgg_win_size)
            self.vgg_win_size = vgg_win_size
            # img_tensor 의 feature 추출. conv 연산을 하려면 [batch, channel, h, w] 형태여야 하기때문에 unsqueeze 사용.
            self.img_tensor_feature = self.Vgg19FeatureExtractor(self.img_tensor.float().unsqueeze(0).cuda()).detach()
            #print('img_tensor_feature.shape :', img_tensor_feature.shape)

            # img_ref_tensor 의 feature 추출.
            self.imgtensor_feature_2scale = self.Vgg19FeatureExtractor(self.img_2scale_tensor.float().unsqueeze(0).cuda()).detach()
            #print('img_ref_tensor_feature.shape :', imgtensor_feature_2scale.shape)
            self.imgtensor_feature_4scale = self.Vgg19FeatureExtractor(self.img_4scale_tensor.float().unsqueeze(0).cuda()).detach()

        else:  # -> 이미지 그대로 사용한다.
            self.vgg_win_size = self.WINDOW_SIZE
            self.img_tensor_feature = self.img_tensor.float().unsqueeze(0)
            self.imgtensor_feature_2scale = self.img_2scale_tensor.float().unsqueeze(0)
            self.imgtensor_feature_4scale = self.img_4scale_tensor.float().unsqueeze(0)
        self.PatchPool_tensor_2scale = RGB_patch_tensors_extractor(self.imgtensor_feature_2scale, self.vgg_win_size)
        self.PatchPool_tensor_4scale = RGB_patch_tensors_extractor(self.imgtensor_feature_4scale, int(self.vgg_win_size/2))
        [_,_,feature_M,feature_N] = self.imgtensor_feature_2scale.shape
        [_,_,feature_M_4scale,feature_N_4scale] = self.imgtensor_feature_4scale.shape

        self.N_BIN = 5
        #self.hist_threshold = hist_Threshold(self.N_BIN)
        self.hist_threshold = [60,100,140,170]

        self.bin = {}  # for x0.5 scale bin
        ################################################################################################

        for i in range(len(self.hist_threshold) + 1): # 각 threshold bin의 빈 배열 선언
            self.bin[i] = []
        for i in range(feature_M):
            for j in range(feature_N):
                current_center_depth = self.depth_scale_img[4 * i][4 * j]
                if current_center_depth < self.hist_threshold[0]:
                    self.bin[0].append([i, j])
                elif current_center_depth >= self.hist_threshold[0] and current_center_depth < self.hist_threshold[1]:
                    self.bin[1].append([i, j])
                elif current_center_depth >= self.hist_threshold[1] and current_center_depth < self.hist_threshold[2]:
                    self.bin[2].append([i, j])
                elif current_center_depth >= self.hist_threshold[2] and current_center_depth < self.hist_threshold[3]:
                    self.bin[3].append([i, j])
                else:
                    self.bin[4].append([i, j])

        #######################################bin 나누기
        print('2scale bin index')
        print(len(self.bin[0]))
        print(len(self.bin[1]))
        print(len(self.bin[2]))
        print(len(self.bin[3]))
        print(len(self.bin[4]))
        #print(len(self.bin[5]))
        self.Pool = {}
        self.Pool_index = {}
        self.reshape_arr = {}
        for k in range(len(self.hist_threshold) + 1):
            self.Pool[k] = []
            self.reshape_arr[k] = []
            for i in range(len(self.bin[k])):
                [idx_i, idx_j] = self.bin[k][i]
                idx_1d = idx2d_to_idx1d(idx_i, idx_j, feature_M, feature_N)
                if idx_1d >= self.PatchPool_tensor_2scale.shape[0]:
                    continue
                reshape = np.reshape(self.PatchPool_tensor_2scale[idx_1d].cpu().detach().numpy(), (-1))
                self.reshape_arr[k].append(reshape)
            km = KMedoids(n_clusters=int(len(self.bin[k]) / self.PATCH_BIN) + 1, init='k-medoids++')
            y_km = km.fit(self.reshape_arr[k])

            self.Pool_index[k] = y_km.medoid_indices_
            for i in range(len(y_km.cluster_centers_)):
                xxxx = np.reshape(y_km.cluster_centers_[i], (256, self.vgg_win_size, self.vgg_win_size))
                self.Pool[k].append(xxxx)

        self.bin_4scale = {}
        for r in range(len(self.hist_threshold) + 1):
            self.bin_4scale[r] = []
            for i in range(len(self.bin[r])):
                kkkidx = [int(self.bin[r][i][0] / scale_factor[0][1]), int(self.bin[r][i][1] / scale_factor[0][0])]
                if kkkidx not in self.bin_4scale[r]:
                    self.bin_4scale[r].append(kkkidx)

        self.Pool_4scale = {}
        self.Pool_index_4scale = {}
        self.reshape_arr_4scale = {}
        for k in range(len(self.hist_threshold) + 1):
            self.Pool_4scale[k] = []
            self.reshape_arr_4scale[k] = []
            for i in range(len(self.bin_4scale[k])):
                [idx_i, idx_j] = self.bin_4scale[k][i]
                idx_1d = idx2d_to_idx1d(idx_i, idx_j, feature_M_4scale, feature_N_4scale)
                if idx_1d >= self.PatchPool_tensor_4scale.shape[0]:
                    continue
                reshape = np.reshape(self.PatchPool_tensor_4scale[idx_1d].cpu().detach().numpy(), (-1))
                self.reshape_arr_4scale[k].append(reshape)
            km = KMedoids(n_clusters=int(len(self.bin_4scale[k]) / self.PATCH_BIN_4scale) + 1, init='k-medoids++')
            y_km = km.fit(self.reshape_arr_4scale[k])

            self.Pool_index_4scale[k] = y_km.medoid_indices_
            for i in range(len(y_km.cluster_centers_)):
                xxxx = np.reshape(y_km.cluster_centers_[i], (256, int(self.vgg_win_size/2), int(self.vgg_win_size/2)))
                self.Pool_4scale[k].append(xxxx)
    def train_run(self, shift_x, shift_y):  # shift_x : width 좌표 shift_y : height 좌표
        ###############################################################################################################################
        coord_depth = self.depth_img[shift_y][shift_x]
        HARD_UPSAMPLING_SWITCH = False
        coord_bin = 0
        min_distance = np.inf
        min_idx = None
        if coord_depth < self.hist_threshold[0]:
            coord_bin = 0
        elif coord_depth >= self.hist_threshold[0] and coord_depth < self.hist_threshold[1]:
            coord_bin = 1
        elif coord_depth >= self.hist_threshold[1] and coord_depth < self.hist_threshold[2]:
            coord_bin = 2
        elif coord_depth >= self.hist_threshold[2] and coord_depth < self.hist_threshold[3]:
            coord_bin = 3
        else:
            coord_bin = 4

        current_patch = self.imgtensor_feature_2scale[..., int(shift_y / (4*self.scale_factor[0][1])):int(shift_y / (4*self.scale_factor[0][1])) + int(self.vgg_win_size/2),
                        int(shift_x / (4*self.scale_factor[0][0])):int(shift_x / (4*self.scale_factor[0][0])) + int(self.vgg_win_size/2)]
        #print(current_patch.shape)
        img_tensor_feature_patch_norm = current_patch / torch.sqrt(torch.sum(current_patch ** 2))

        all_tensor_patch = []

        if coord_bin == 0:
            for k in range(1, 5, 1):
                for i in range(len(self.Pool_4scale[k])):
                    tensor_patch = torch.from_numpy(self.Pool_4scale[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 1:
            for k in range(2, 5, 1):
                for i in range(len(self.Pool_4scale[k])):
                    tensor_patch = torch.from_numpy(self.Pool_4scale[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 2:
            for k in range(3, 5, 1):
                for i in range(len(self.Pool_4scale[k])):
                    tensor_patch = torch.from_numpy(self.Pool_4scale[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 3:
            for k in range(4, 5, 1):
                for i in range(len(self.Pool_4scale[k])):
                    tensor_patch = torch.from_numpy(self.Pool_4scale[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 4:
            for k in range(4, 5, 1):
                for i in range(len(self.Pool_4scale[k])):
                    tensor_patch = torch.from_numpy(self.Pool_4scale[k][i])
                    all_tensor_patch.append(tensor_patch)

        """
        for i in range(len(self.Pool_4scale[coord_bin])):
            tensor_patch = torch.from_numpy(self.Pool_4scale[coord_bin][i])
            all_tensor_patch.append(tensor_patch)
        """
        stacked_tensor_patch = torch.stack(all_tensor_patch)
        _pool_patches_tensor = torch.sqrt(torch.sum(stacked_tensor_patch ** 2, dim=(1, 2, 3)))
        pool_size = stacked_tensor_patch.shape[0]
        _pool_patches_tensor = _pool_patches_tensor.view(pool_size, 1, 1, 1)
        pool_patches_tensor_norm = stacked_tensor_patch / _pool_patches_tensor.expand(stacked_tensor_patch.shape)
        _input = img_tensor_feature_patch_norm  # [1, C, win_size, win_size]
        _weight = pool_patches_tensor_norm  # [pool_size, C, win_size, win_size]

        with torch.no_grad():  # grad 는 여기서 필요 없기때문에 빠른 속도를 위해 꺼준다.
            similarity_score = F.conv2d(_input.cuda(), _weight.cuda())
        # 각 pool 의 영상이 img 와 얼마나 비슷한지에 대한 점수 리스트.
        similarity_score = torch.squeeze(similarity_score)
        similarity_score_argmax = torch.argmax(similarity_score)  # (1, pool_size, 1, 1)
        #print('similarity_score_MAX : ',torch.max(similarity_score))
        #print('similarity_score_argmax :', similarity_score_argmax)
        if torch.max(similarity_score) < self.HARD_THRESHOLD:
            #print('HARD')
            HARD_UPSAMPLING_SWITCH = True
            return 0, 0, HARD_UPSAMPLING_SWITCH
        if coord_bin == 0:
            if similarity_score_argmax < len(self.Pool_4scale[1]):
                coord_bin = 1
                real_similarity_score_argmax = similarity_score_argmax
            elif similarity_score_argmax >= len(self.Pool_4scale[1]) and similarity_score_argmax < len(
                    self.Pool_4scale[1]) + len(self.Pool_4scale[2]):
                coord_bin = 2
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool_4scale[1])
            elif similarity_score_argmax >= len(self.Pool_4scale[1]) + len(
                    self.Pool_4scale[2]) and similarity_score_argmax < len(self.Pool_4scale[1]) + len(
                    self.Pool_4scale[2]) + len(self.Pool_4scale[3]):
                coord_bin = 3
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool_4scale[1]) - len(
                    self.Pool_4scale[2])
            else:
                coord_bin = 4
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool_4scale[1]) - len(
                    self.Pool_4scale[2]) - len(self.Pool_4scale[3])
        elif coord_bin == 1:
            if similarity_score_argmax < len(self.Pool_4scale[2]):
                coord_bin = 2
                real_similarity_score_argmax = similarity_score_argmax
            elif similarity_score_argmax >= len(self.Pool_4scale[2]) and similarity_score_argmax < len(
                    self.Pool_4scale[2]) + len(self.Pool_4scale[3]):
                coord_bin = 3
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool_4scale[2])
            else:
                coord_bin = 4
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool_4scale[2]) - len(
                    self.Pool_4scale[3])
        elif coord_bin == 2:
            if similarity_score_argmax < len(self.Pool_4scale[3]):
                coord_bin = 3
                real_similarity_score_argmax = similarity_score_argmax
            else:
                coord_bin = 4
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool_4scale[3])

        elif coord_bin == 3:
            coord_bin = 4
            real_similarity_score_argmax = similarity_score_argmax
        elif coord_bin == 4:
            coord_bin = 4
            real_similarity_score_argmax = similarity_score_argmax

        [REAL_I, REAL_J] = self.bin_4scale[coord_bin][self.Pool_index_4scale[coord_bin][real_similarity_score_argmax]]
        ###################################################comparison
        return 4*REAL_I, 4*REAL_J, HARD_UPSAMPLING_SWITCH

    def test_run(self, shift_x, shift_y):
        ################################patch clustering
        HARD_UPSAMPLING_SWITCH=False
        coord_depth = self.depth_img[shift_y][shift_x]
        coord_bin = 0
        min_distance = np.inf
        min_idx = None
        if coord_depth < self.hist_threshold[0]:
            coord_bin = 0
        elif coord_depth >= self.hist_threshold[0] and coord_depth < self.hist_threshold[1]:
            coord_bin = 1
        elif coord_depth >= self.hist_threshold[1] and coord_depth < self.hist_threshold[2]:
            coord_bin = 2
        elif coord_depth >= self.hist_threshold[2] and coord_depth < self.hist_threshold[3]:
            coord_bin = 3
        else:
            coord_bin = 4

        current_patch = self.img_tensor_feature[..., int(shift_y / 4):int(shift_y / 4) + self.vgg_win_size,
                        int(shift_x / 4):int(shift_x / 4) + self.vgg_win_size]
        #print(current_patch.shape)
        img_tensor_feature_patch_norm = current_patch / torch.sqrt(torch.sum(current_patch ** 2))

        all_tensor_patch = []

        if coord_bin == 0:
            for k in range(1, 5, 1):
                for i in range(len(self.Pool[k])):
                    tensor_patch = torch.from_numpy(self.Pool[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 1:
            for k in range(2, 5, 1):
                for i in range(len(self.Pool[k])):
                    tensor_patch = torch.from_numpy(self.Pool[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 2:
            for k in range(3, 5, 1):
                for i in range(len(self.Pool[k])):
                    tensor_patch = torch.from_numpy(self.Pool[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 3:
            for k in range(4, 5, 1):
                for i in range(len(self.Pool[k])):
                    tensor_patch = torch.from_numpy(self.Pool[k][i])
                    all_tensor_patch.append(tensor_patch)
        if coord_bin == 4:
            for k in range(4, 5, 1):
                for i in range(len(self.Pool[k])):
                    tensor_patch = torch.from_numpy(self.Pool[k][i])
                    all_tensor_patch.append(tensor_patch)
        """
        for i in range(len(self.Pool[coord_bin])):
            tensor_patch = torch.from_numpy(self.Pool[coord_bin][i])
            all_tensor_patch.append(tensor_patch)
        """
        stacked_tensor_patch = torch.stack(all_tensor_patch)
        _pool_patches_tensor = torch.sqrt(torch.sum(stacked_tensor_patch ** 2, dim=(1, 2, 3)))
        pool_size = stacked_tensor_patch.shape[0]
        _pool_patches_tensor = _pool_patches_tensor.view(pool_size, 1, 1, 1)
        pool_patches_tensor_norm = stacked_tensor_patch / _pool_patches_tensor.expand(stacked_tensor_patch.shape)
        _input = img_tensor_feature_patch_norm  # [1, C, win_size, win_size]
        _weight = pool_patches_tensor_norm  # [pool_size, C, win_size, win_size]

        with torch.no_grad():  # grad 는 여기서 필요 없기때문에 빠른 속도를 위해 꺼준다.
            similarity_score = F.conv2d(_input.cuda(), _weight.cuda())
        # 각 pool 의 영상이 img 와 얼마나 비슷한지에 대한 점수 리스트.
        similarity_score = torch.squeeze(similarity_score)
        similarity_score_argmax = torch.argmax(similarity_score)  # (1, pool_size, 1, 1)
        if torch.max(similarity_score) < self.HARD_THRESHOLD:
            #print('HARD')
            HARD_UPSAMPLING_SWITCH = True
            return 0, 0, HARD_UPSAMPLING_SWITCH

        #print('similarity_score_argmax :', similarity_score_argmax)
        if coord_bin == 0:
            if similarity_score_argmax < len(self.Pool[1]):
                coord_bin = 1
                real_similarity_score_argmax = similarity_score_argmax
            elif similarity_score_argmax >= len(self.Pool[1]) and similarity_score_argmax < len(self.Pool[1]) + len(
                    self.Pool[2]):
                coord_bin = 2
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool[1])
            elif similarity_score_argmax >= len(self.Pool[1]) + len(self.Pool[2]) and similarity_score_argmax < len(
                    self.Pool[1]) + len(self.Pool[2]) + len(self.Pool[3]):
                coord_bin = 3
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool[1]) - len(self.Pool[2])
            else:
                coord_bin = 4
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool[1]) - len(self.Pool[2]) - len(
                    self.Pool[3])
        elif coord_bin == 1:
            if similarity_score_argmax < len(self.Pool[2]):
                coord_bin = 2
                real_similarity_score_argmax = similarity_score_argmax
            elif similarity_score_argmax >= len(self.Pool[2]) and similarity_score_argmax < len(self.Pool[2]) + len(
                    self.Pool[3]):
                coord_bin = 3
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool[2])
            else:
                coord_bin = 4
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool[2]) - len(self.Pool[3])
        elif coord_bin == 2:
            if similarity_score_argmax < len(self.Pool[3]):
                coord_bin = 3
                real_similarity_score_argmax = similarity_score_argmax
            else:
                coord_bin = 4
                real_similarity_score_argmax = similarity_score_argmax - len(self.Pool[3])
        elif coord_bin == 3:
            coord_bin = 4
            real_similarity_score_argmax = similarity_score_argmax
        elif coord_bin == 4:
            coord_bin = 4
            real_similarity_score_argmax = similarity_score_argmax

        [REAL_I, REAL_J] = self.bin[coord_bin][self.Pool_index[coord_bin][real_similarity_score_argmax]]
        ###################################################comparison

        return 8*REAL_I, 8*REAL_J, HARD_UPSAMPLING_SWITCH



class Patchmatch_VGG_MATCHING_ALL_SEARCH:
    def __init__(self, depth_img, input_img, crop_size):
        """
        depth_img = input_img 입력 영상
        """
        self.crop_size = crop_size
        self.depth_img = depth_img
        [M, N] = self.depth_img.shape  # image size [ Height , Width ]
        self.depth_scale_img = cv2.resize(self.depth_img, (int(N / 2), int(M / 2)), cv2.INTER_CUBIC)  # x0.5 scale depth image

        self.WINDOW_SIZE = int(self.crop_size/2)-1  # Patch size
        self.WINDOW_PAD = int(self.WINDOW_SIZE/2) - 1  # Padding size
        self.HARD_THRESHOLD = 0.01
        self.input = input_img
        self.img_scale = cv2.resize(input_img,(int(N/2),int(M/2)),cv2.INTER_CUBIC) # x0.5 scale image for making patches
        ##########
        [M_scale, N_scale] = self.depth_scale_img.shape # 1/2 image shape
        #############################################3
        self.img_4scale = cv2.resize(input_img, (int(N/4), int(M/4)), cv2.INTER_CUBIC)

        totensor = transforms.ToTensor()
        self.img_tensor = totensor(self.input)
        self.img_2scale_tensor = totensor(self.img_scale)
        self.img_4scale_tensor = totensor(self.img_4scale)

        self.vgg_range = 12
        if self.vgg_range > 0:  # -> vgg feature 를 뽑는다.
            # Vgg19FeatureExtractor 를 선언하는 것 만으로도 gpu 용량을 차지하는 듯 하다?
            self.Vgg19FeatureExtractor = Vgg19FeatureExtractor(vgg_range=self.vgg_range, rgb_range=1).cuda()

            # vgg feature 에서 win size 를 즉정한다. 측정을 위해 win_size 크기의 임의의 패치를 모델에 통과시켜서 output size 를 관찰한다.
            temp_patch = torch.zeros(3, self.WINDOW_SIZE, self.WINDOW_SIZE)  # vgg19 의 입력은 3 channel 이다.
            print('temp_patch.shape :', temp_patch.shape)
            vgg_win_size = self.Vgg19FeatureExtractor(temp_patch.unsqueeze(0).cuda()).shape[-1]
            if vgg_win_size % 2 == 0:  # win size 가 홀수인게 편한 듯 하다.-> 짝수면 홀수로 바꿔줌.
                vgg_win_size += 1
            #print('vgg_win_size :', vgg_win_size)
            self.vgg_win_size = vgg_win_size
            # img_tensor 의 feature 추출. conv 연산을 하려면 [batch, channel, h, w] 형태여야 하기때문에 unsqueeze 사용.
            self.img_tensor_feature = self.Vgg19FeatureExtractor(self.img_tensor.float().unsqueeze(0).cuda()).detach()
            #print('img_tensor_feature.shape :', img_tensor_feature.shape)

            # img_ref_tensor 의 feature 추출.
            self.imgtensor_feature_2scale = self.Vgg19FeatureExtractor(self.img_2scale_tensor.float().unsqueeze(0).cuda()).detach()
            #print('img_ref_tensor_feature.shape :', imgtensor_feature_2scale.shape)
            self.imgtensor_feature_4scale = self.Vgg19FeatureExtractor(self.img_4scale_tensor.float().unsqueeze(0).cuda()).detach()

        else:  # -> 이미지 그대로 사용한다.
            self.vgg_win_size = self.WINDOW_SIZE
            self.img_tensor_feature = self.img_tensor.float().unsqueeze(0)
            self.imgtensor_feature_2scale = self.img_2scale_tensor.float().unsqueeze(0)
            self.imgtensor_feature_4scale = self.img_4scale_tensor.float().unsqueeze(0)
        self.PatchPool_tensor_2scale = RGB_patch_tensors_extractor(self.imgtensor_feature_2scale, self.vgg_win_size)
        self.PatchPool_tensor_4scale = RGB_patch_tensors_extractor(self.imgtensor_feature_4scale, int(self.vgg_win_size/2))
        [_,_,feature_M,feature_N] = self.imgtensor_feature_2scale.shape
        [_,_,feature_M_4scale,feature_N_4scale] = self.imgtensor_feature_4scale.shape


    def train_run(self, shift_x, shift_y):  # shift_x : width 좌표 shift_y : height 좌표
        ###############################################################################################################################
        HARD_UPSAMPLING_SWITCH = False
        [_, _, feature_M_4scale, feature_N_4scale] = self.imgtensor_feature_4scale.shape
        current_patch = self.imgtensor_feature_2scale[..., int(shift_y / 8):int(shift_y / 8) + int(self.vgg_win_size/2),
                        int(shift_x / 8):int(shift_x / 8) + int(self.vgg_win_size/2)]
        #print(current_patch.shape)
        img_tensor_feature_patch_norm = current_patch / torch.sqrt(torch.sum(current_patch ** 2))

        all_tensor_patch = self.PatchPool_tensor_4scale

        stacked_tensor_patch = (all_tensor_patch)
        _pool_patches_tensor = torch.sqrt(torch.sum(stacked_tensor_patch ** 2, dim=(1, 2, 3)))
        pool_size = stacked_tensor_patch.shape[0]
        _pool_patches_tensor = _pool_patches_tensor.view(pool_size, 1, 1, 1)
        pool_patches_tensor_norm = stacked_tensor_patch / _pool_patches_tensor.expand(stacked_tensor_patch.shape)
        _input = img_tensor_feature_patch_norm  # [1, C, win_size, win_size]
        _weight = pool_patches_tensor_norm  # [pool_size, C, win_size, win_size]

        with torch.no_grad():  # grad 는 여기서 필요 없기때문에 빠른 속도를 위해 꺼준다.
            similarity_score = F.conv2d(_input.cuda(), _weight.cuda())
        # 각 pool 의 영상이 img 와 얼마나 비슷한지에 대한 점수 리스트.
        similarity_score = torch.squeeze(similarity_score)
        similarity_score_argmax = torch.argmax(similarity_score)  # (1, pool_size, 1, 1)
        similarity_score_argmax = similarity_score_argmax.cpu().detach().numpy()
        #print('similarity_score_MAX : ',torch.max(similarity_score))
        #print('similarity_score_argmax :', similarity_score_argmax)
        if torch.max(similarity_score) < self.HARD_THRESHOLD:
            #print('HARD')
            HARD_UPSAMPLING_SWITCH = True
            return 0, 0, HARD_UPSAMPLING_SWITCH

        [REAL_I, REAL_J] = idx1d_to_idx2d(similarity_score_argmax,feature_M_4scale,feature_N_4scale)
        ###################################################comparison
        return 4*REAL_I, 4*REAL_J, HARD_UPSAMPLING_SWITCH

    def test_run(self, shift_x, shift_y):
        ################################patch clustering
        [_,_,feature_M,feature_N] = self.imgtensor_feature_2scale.shape
        HARD_UPSAMPLING_SWITCH=False
        current_patch = self.img_tensor_feature[..., int(shift_y / 4):int(shift_y / 4) + self.vgg_win_size,
                        int(shift_x / 4):int(shift_x / 4) + self.vgg_win_size]
        #print(current_patch.shape)
        img_tensor_feature_patch_norm = current_patch / torch.sqrt(torch.sum(current_patch ** 2))

        all_tensor_patch = self.PatchPool_tensor_2scale

        stacked_tensor_patch = (all_tensor_patch)
        _pool_patches_tensor = torch.sqrt(torch.sum(stacked_tensor_patch ** 2, dim=(1, 2, 3)))
        pool_size = stacked_tensor_patch.shape[0]
        _pool_patches_tensor = _pool_patches_tensor.view(pool_size, 1, 1, 1)
        pool_patches_tensor_norm = stacked_tensor_patch / _pool_patches_tensor.expand(stacked_tensor_patch.shape)
        _input = img_tensor_feature_patch_norm  # [1, C, win_size, win_size]
        _weight = pool_patches_tensor_norm  # [pool_size, C, win_size, win_size]

        with torch.no_grad():  # grad 는 여기서 필요 없기때문에 빠른 속도를 위해 꺼준다.
            similarity_score = F.conv2d(_input.cuda(), _weight.cuda())
        # 각 pool 의 영상이 img 와 얼마나 비슷한지에 대한 점수 리스트.
        similarity_score = torch.squeeze(similarity_score)
        similarity_score_argmax = torch.argmax(similarity_score)  # (1, pool_size, 1, 1)
        similarity_score_argmax = similarity_score_argmax.cpu().detach().numpy()

        if torch.max(similarity_score) < self.HARD_THRESHOLD:
            #print('HARD')
            HARD_UPSAMPLING_SWITCH = True
            return 0, 0, HARD_UPSAMPLING_SWITCH

        [REAL_I, REAL_J] = idx1d_to_idx2d(similarity_score_argmax,feature_M,feature_N)
        ###################################################comparison

        return 8*REAL_I, 8*REAL_J, HARD_UPSAMPLING_SWITCH
